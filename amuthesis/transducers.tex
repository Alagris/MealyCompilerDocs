


The theory behind implementation and design of Solomonoff has been studied and developed over the course of 2 years. The early versions and our initial ideas looked very different to the final results we've achieved. 

At the beginning it was meant to be a simple tools that focused only on deterministic Mealy automata. Such a model was very limited and it quickly became apparent that certain extensions would need to be made. In the end we implemented a compiler that supports nondeterministic functional weighted symbolic transducers. 

\section{Mealy automata and transducers}

The standard definition of automaton found in introductory courses states that finite state automaton is  a tuple $(Q,I,\delta,F,\Sigma)$ where $Q$ is the set of states, $I \subset Q$ are the initial states,  $F \subset Q$ are the final (or accepting) states, $\delta:Q\times \Sigma \rightarrow Q$ is the transition function and $\Sigma$ is some alphabet. 

A Mealy machine extends the above definition with output  $(Q,I,\delta,F,\Sigma,\Gamma)$ where $\Gamma$ is some output alphabet and transition function has the form of $\delta:Q\times \Sigma \rightarrow Q \times \Gamma$. 

Such a model is frequently used in the field of formal methods. Many complex state-based systems can be modelled and simplified using Mealy machines. As an example consider a black-box computer program whose logs can be observed. The current snapshot of the program's memory determines its state. Depending on subsequent user input, we might observe different log traces. There are many existing machine learning and inference algorithms that can build an equivalent model of Mealy machine only by interacting with such a black-box system and reading its logs. 

It can be proved that the expressive power of deterministic automata with output is strictly less than that of their nondeterministic counterparts. It is known as the prefix-preserving  property. If a deterministic automaton read input string $\sigma_1\sigma_2\sigma_1$ and printed $\gamma_1\gamma_1\gamma_2$, then at the next step it is only allowed to append one more symbol to the previously generated output. For instance we could not suddenly change the output to $\gamma_2\gamma_2\gamma_2\gamma_1$ after reading one more input symbol $\sigma_1\sigma_2\sigma_1\sigma_2$. The prefix $\gamma_1\gamma_1\gamma_2$ must be preserved. 

The problems that we wanted to tackle with Solomonoff revolved around building sequence-to-sequence models. For instance we might want to translate from numbers written as English words into digits. A sentence like "one apple" should become "1 apple". The prefix preserving property would be too limiting because it often happens that the suffix of string has decisive impact on the translation. The phrase "once again"
also starts with prefix "one" but it should not be translated as "1ce again"!

We intended to find the right balance between expressive power of nondeterministic machines and strong formal guarantees of Mealy automata. To achieve this, we initially decided to use multitape automata. The idea was to write all possible continuations of given output and store each in a separate output tape. Then upon reaching the end of string, the state would decide which tape to use as output. 

Over the course of research and development we have discovered that the power of multitape Mealy machines was still too limiting for our purposes. In particular we could define congruence classes on pairs of strings similar to those in Myhill-Nerode theorem. Then it can be easily noticed that as long as the number of output tapes is finite, the number of congruence classes must be finite as well. A very simple counter-example would the language that for every string $a$, $aa$, $aaa$,... respectively prints output $c$, $bc$, $bbc$,... It could not be expresses using only a finite number of output tapes, because there are infinitely many ways to continue the output and none of them is a prefix of the other.  



Yet another limitation of Mealy machines is that their $\delta$ function enforces outputs of the exact same length as inputs. In the field of natural language processing such an assumption is too strict. For instance, we might want to 
build a machine that translates sentences from one language to another. A word "fish" in English might have 4 letters but Spanish "pescado" is much longer. Our first idea was to use sequential transducers instead of the plain Mealy automata. Their definition allows the transition function to print output strings of arbitrary length $\delta:Q\times \Sigma \rightarrow Q^*$. 


The nondeterministic transducers  don't suffer from any of the above problems. Their expressive power exactly corresponds to that of regular transductions. It's a very strong and expressive class. The only way to obtain a stronger model would be by introducing context-free grammars and pushdown automata. The reason why we were hesitant to use this approach was because of the possible ambiguity of output. Nondeterministic transducers may contain epsilon transitions, which could lead to infinite number of outputs for a any given input. Without epsilons, the number of outputs is finite but it's still possible to return more than one ambiguous output.

The model of automata that was finally implemented in Solomonoff turned out to be the functional nondeterministic transducers. Functionality means that for any input, there may be at most one output. Such a  model proved to provide the perfect balance of power with many strong formal guarantees. While epsilon transitions strictly increase power of transducers, when restricted only to functional automata, the erasure of epsilons becomes possible. Using advance-and-delay algorithm one can test functionality of any automaton in quadratic time. There exists a special version of powerset construction that can take any functional transducers and produce an equivalent unambiguous automaton. One can take advantage of unambiguity to build an inference algorithm for learning functional automata from sample data.  The automata are closed under union, concatenation, Kleene closure and composition. Unlike nonfunctional transducers, they are not closed under inversion but we developed a special algebra that uniquely defines an invertible  bijective transduction for any automaton. Glushkov's construction can be augmented to produce functional transducers.  Lexicographic semiring of weights can be used to make functional automata more compact.  


Once we decided to use the power of functional transducers, the next problem we had to solve was their optimisation. One of the most important operations in natural language processing is the context-dependent rewrite. The standard way of implementing it is by building a large transducer that handles all possible cases.
In Solomonoff we've developed our own approach that produces much smaller automata. It is done with lexicographic weights.

Another important optimisation is the state minimisation. Many existing libraries implement separate functions for all regular operations and additional one for minimisation. A regular expression like $A(B+C)*$ would be the translated to a series of function calls like 
\[
minimise(concatenate(A,kleene\_closure(union(B,C))))
\]

In Solomonoff we don't have separate implementation for those operations. Instead everything is integrated in form of one monolithic procedure that implements Glushkov's construction. This way the produced automata are very small even without the need for minimisation. If the regular expression consists of $n$ input symbols, then the resulting transducers has $n+1$ states. Because there is one-to-one correspondence between regular expression symbols and automata states, we are able to retain plenty of useful meta information. In particular each state can tell us exactly which source file it comes from and the precise text line and column in that file.  
This way, whenever compilation fails, user can see meaningful error messages.


The standard minimisation procedure used by most libraries works by finding the unique smallest deterministic automaton. Nondeterministic automata do not admit unique smallest representative and might be even exponentially smaller than their equivalent minimal deterministic counterparts. Finding the smallest possible nondeterministic automaton is a hard problem. For this reason Solomonoff implements a heuristic pseudo-minimisation algorithm that attempts to compress nondeterministic transducers and does not attempt to determinise them. Glushkov's construction already produces very small automata, hence any attempt at minimising them by determinisation would result in larger automata than the initial ones.

Lexicographic weights allow the make the transducers even more compact. We proved that there exist weighted automata exponentially smaller than even the smallest unweighted nondeterministic ones. 

In the tasks of natural language processing its common to work with large alphabets. User input might contain unexpected sequences like math symbols, foreign words or even emojis and other UNICODE entities. The regular expressions should handle such cases gracefully, especially when using wildcards such as \texttt{.*} or \texttt{\textbackslash p\{Lu\}}. Representation of large character classes is a challenging task for finite state automata. In order to use the dot wildcard in UNICODE, the automaton would require millions of transitions, one for each individual symbol. 
In order to optimise this, Solomonoff employs symbolic transitions. While classic automata have edges labelled with individual symbols, our transducers have edges that span entire ranges. A range is easy to encode in computers memory. It's enough to store the first and last symbol. 

Compilation of transducers is the core part of our library but in order to make the automata useful there must be a way to execute them. Deterministic transducers and Mealy machines could be evaluated in linear time. Nondeterministic automata are more expensive. The computation might branch and automaton could be in multiple states simultaneously. Using dynamic programming it's possible to build a table with rows representing symbol on input string and each column keeping track of one state. 
At each step of execution, one input symbol is read and one row in the table is filled based on the contents of previous row. By the end of evaluating such a table, it's enough to scan the last row to find a column of some accepting state and then the table can be backtracked from that state backwards. The backtracking step is necessary to correct output produced from each transition. 

This algorithm has been made even more efficient by using techniques from graph theory. Every automaton is a directed graph that could be represented as adjacency matrix. Sparse graphs can be optimised and instead of using matrix it's possible to only store the list of adjacent vertices.  Automata very often are a perfect example of sparse graphs. Hence instead of using a table, it's better to use a list of states nondeterministically reached at any given step of evaluation. The backtracking can be made efficient by storing pointer to source state taken at any transition.

Glushkov's construction relies on building three sets: the set of initial symbols, final symbols and 2-factor strings. The prototype version of Solomonoff would represent sets as bitsets, where each bit specifies whether element belongs to he set or not. This representation simplified implementation of many algorithms but was highly inefficient. The current implementation uses hash sets instead. While hash maps have constant insertions and deletions, they ensure it ad the cost of larger memory consumption. During benchmarks on real-life datasets, Solomonoff would often run out of memory and crash. Further optimisation was obtained by representing sparse sets as arrays of elements. This approach proved to be the best choice, because Glushkov's construction inherently ensures uniqueness of all inserted elements (hence using hashes to search for duplicates before insertion was not necessary)  and it does not use deletions. As a result any array was guaranteed to behave like a set.

The standard definition of Glushkov's construction produces set of 2-factors as its output. Then a separate procedure would be necessary to collect all such strings and convert them into a graph of automaton. We found a way to make it more efficient and build the graph directly. The step of building 2-factors was entirely bypassed. This provided even further optimisation.

One of the core features of Solomonoff is the algorithm for detecting ambiguous nondeterminism. Advance-and-delay procedure can check functionality of automaton in quadratic time. While the compiler does provide implementation of this procedure, it is not used for checking functionality. Instead we use a simpler algorithm for checking strong functionality of lexicographic weights. While the performance difference between the two is negligable, the advantage of our approach comes from  better error messages in case of nondeterminism. If some lexicographic weights are in conflict with each other, compiler can point user to the exact line and column of text where the conflict arises, whereas advance-and-delay might miss the origin of problem and only fail further down the line.















